{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ad7d96-f575-4c06-a798-41c300634e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters (should list layer1–4 and fc):\n",
      "  layer1.0.conv1.weight\n",
      "  layer1.0.bn1.weight\n",
      "  layer1.0.bn1.bias\n",
      "  layer1.0.conv2.weight\n",
      "  layer1.0.bn2.weight\n",
      "  layer1.0.bn2.bias\n",
      "  layer1.1.conv1.weight\n",
      "  layer1.1.bn1.weight\n",
      "  layer1.1.bn1.bias\n",
      "  layer1.1.conv2.weight\n",
      "  layer1.1.bn2.weight\n",
      "  layer1.1.bn2.bias\n",
      "  layer2.0.conv1.weight\n",
      "  layer2.0.bn1.weight\n",
      "  layer2.0.bn1.bias\n",
      "  layer2.0.conv2.weight\n",
      "  layer2.0.bn2.weight\n",
      "  layer2.0.bn2.bias\n",
      "  layer2.0.downsample.0.weight\n",
      "  layer2.0.downsample.1.weight\n",
      "  layer2.0.downsample.1.bias\n",
      "  layer2.1.conv1.weight\n",
      "  layer2.1.bn1.weight\n",
      "  layer2.1.bn1.bias\n",
      "  layer2.1.conv2.weight\n",
      "  layer2.1.bn2.weight\n",
      "  layer2.1.bn2.bias\n",
      "  layer3.0.conv1.weight\n",
      "  layer3.0.bn1.weight\n",
      "  layer3.0.bn1.bias\n",
      "  layer3.0.conv2.weight\n",
      "  layer3.0.bn2.weight\n",
      "  layer3.0.bn2.bias\n",
      "  layer3.0.downsample.0.weight\n",
      "  layer3.0.downsample.1.weight\n",
      "  layer3.0.downsample.1.bias\n",
      "  layer3.1.conv1.weight\n",
      "  layer3.1.bn1.weight\n",
      "  layer3.1.bn1.bias\n",
      "  layer3.1.conv2.weight\n",
      "  layer3.1.bn2.weight\n",
      "  layer3.1.bn2.bias\n",
      "  layer4.0.conv1.weight\n",
      "  layer4.0.bn1.weight\n",
      "  layer4.0.bn1.bias\n",
      "  layer4.0.conv2.weight\n",
      "  layer4.0.bn2.weight\n",
      "  layer4.0.bn2.bias\n",
      "  layer4.0.downsample.0.weight\n",
      "  layer4.0.downsample.1.weight\n",
      "  layer4.0.downsample.1.bias\n",
      "  layer4.1.conv1.weight\n",
      "  layer4.1.bn1.weight\n",
      "  layer4.1.bn1.bias\n",
      "  layer4.1.conv2.weight\n",
      "  layer4.1.bn2.weight\n",
      "  layer4.1.bn2.bias\n",
      "  fc.weight\n",
      "  fc.bias\n",
      "Epoch 1/10  Train L: 1.5183, Acc: 0.4121  |  Val L: 1.2809, Acc: 0.5132\n",
      "Epoch 2/10  Train L: 1.2513, Acc: 0.5258  |  Val L: 1.1636, Acc: 0.5656\n",
      "Epoch 3/10  Train L: 1.1548, Acc: 0.5632  |  Val L: 1.1088, Acc: 0.5812\n",
      "Epoch 4/10  Train L: 1.0955, Acc: 0.5857  |  Val L: 1.0611, Acc: 0.6007\n",
      "Epoch 5/10  Train L: 1.0401, Acc: 0.6080  |  Val L: 1.0398, Acc: 0.6082\n",
      "Epoch 6/10  Train L: 0.9990, Acc: 0.6273  |  Val L: 1.0263, Acc: 0.6144\n",
      "Epoch 7/10  Train L: 0.9708, Acc: 0.6374  |  Val L: 1.0054, Acc: 0.6272\n",
      "Epoch 8/10  Train L: 0.9353, Acc: 0.6491  |  Val L: 1.0033, Acc: 0.6272\n",
      "Epoch 9/10  Train L: 0.9078, Acc: 0.6606  |  Val L: 0.9897, Acc: 0.6308\n",
      "Epoch 10/10  Train L: 0.8787, Acc: 0.6736  |  Val L: 0.9829, Acc: 0.6381\n",
      "Best Val Acc: 0.6381 (weights saved to full_resnet_emotion.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748880479.764135 19338412 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748880479.789641 19913590 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running static FER-2013 test batch check ===\n",
      "Sample 0: GT=Angry    Pred=Angry    Probs=[0.46 0.03 0.18 0.   0.11 0.   0.22]\n",
      "Sample 1: GT=Surprise  Pred=Sad      Probs=[0.07 0.   0.14 0.02 0.67 0.03 0.07]\n",
      "Sample 2: GT=Neutral  Pred=Neutral  Probs=[0.18 0.   0.11 0.01 0.31 0.   0.39]\n",
      "Sample 3: GT=Sad      Pred=Sad      Probs=[0.02 0.   0.22 0.   0.54 0.   0.21]\n",
      "Sample 4: GT=Fear     Pred=Sad      Probs=[0.15 0.12 0.18 0.02 0.31 0.14 0.09]\n",
      "Static‐batch accuracy on FER-2013 test batch: 65.62%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Abeni's Real-Time Emotion Detection Model made with PyTorch, Mediapipe, ResNet-18 and OpenCV\n",
    "# ---------------------------\n",
    "\n",
    "# =============================================\n",
    "# 1. All Imports\n",
    "# =============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# =============================================\n",
    "# 2. OpenCV-based preprocessing function\n",
    "# =============================================\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    CLAHE + Adaptive Thresholding: To normalize contrast and then binarize to reduce lighting/occlusion effects.\n",
    "    \"\"\"\n",
    "    # Converting to grayscale\n",
    "    gray  = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Applying CLAHE histogram equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    eq    = clahe.apply(gray)\n",
    "    # Applying adaptive thresholding on the CLAHE result\n",
    "    th = cv2.adaptiveThreshold(\n",
    "        eq,\n",
    "        maxValue=255,\n",
    "        adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        thresholdType=cv2.THRESH_BINARY,\n",
    "        blockSize=11,\n",
    "        C=2\n",
    "    )\n",
    "    # Converting back to 3‐channel RGB\n",
    "    return cv2.cvtColor(th, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# =============================================\n",
    "# 3. Dataset Definition\n",
    "# =============================================\n",
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, csv_file, usage=\"Training\", transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data[self.data[\"Usage\"] == usage].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row    = self.data.iloc[idx]\n",
    "        pixels = np.fromstring(row[\"pixels\"], dtype=int, sep=' ')\n",
    "        img    = pixels.reshape(48,48).astype(np.uint8)\n",
    "        img    = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        img    = preprocess_image(img)           # CLAHE + Adaptive Thresholding\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = int(row[\"emotion\"])\n",
    "        return img, label\n",
    "        \n",
    "# =============================================\n",
    "# 4. Data Transforms\n",
    "# =============================================\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "val_tfm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# 5. Data Loaders\n",
    "# =============================================\n",
    "train_ds  = FER2013Dataset('fer2013.csv', usage='Training',   transform=train_tfm)\n",
    "val_ds    = FER2013Dataset('fer2013.csv', usage='PublicTest', transform=val_tfm)\n",
    "test_ds   = FER2013Dataset('fer2013.csv', usage='PrivateTest',transform=val_tfm)\n",
    "\n",
    "train_loader = DataLoader(train_ds,  batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,    batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# =============================================\n",
    "# 6. Model Definition (unfreezing layers 1 to 4 + fc)\n",
    "# =============================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Unfreeze layer1, layer2, layer3, layer4, and fc\n",
    "for name, param in model.named_parameters():\n",
    "    if not (\n",
    "        name.startswith('layer1') or\n",
    "        name.startswith('layer2') or\n",
    "        name.startswith('layer3') or\n",
    "        name.startswith('layer4') or\n",
    "        name.startswith('fc')\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Replace final fc (1000 -> 7)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 7)\n",
    "model    = model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable parameters (should list layer1–4 and fc):\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name}\")\n",
    "\n",
    "# =============================================\n",
    "# 7. Training Setup (separate LRs + scheduler)\n",
    "# =============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params_to_update = [\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.fc.parameters(),     'lr': 1e-4}\n",
    "]\n",
    "optimizer = optim.Adam(params_to_update)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# =============================================\n",
    "# 8. Training Loop (fine‐tuning all of layers 1 to 4 + fc)\n",
    "# =============================================\n",
    "def train_full_resnet(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss, running_corr = 0.0, 0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss    += loss.item() * inputs.size(0)\n",
    "            running_corr    += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc  = running_corr / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_corr = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss    = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corr += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc  = val_corr / len(val_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs}  \"\n",
    "            f\"Train L: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}  |  \"\n",
    "            f\"Val L: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'full_resnet_emotion.pth')\n",
    "\n",
    "    print(f\"Best Val Acc: {best_acc:.4f} (weights saved to full_resnet_emotion.pth)\")\n",
    "\n",
    "# =============================================\n",
    "# 9. Run Training & Save Weights\n",
    "# =============================================\n",
    "# To remove any outdated checkpoints so we retrain from scratch\n",
    "for old_fn in ['only_fc_emotion_resnet.pth', 'layer2_to_fc_emotion_resnet.pth', 'full_resnet_emotion.pth']:\n",
    "    if os.path.exists(old_fn):\n",
    "        os.remove(old_fn)\n",
    "\n",
    "checkpoint_path = 'full_resnet_emotion.pth'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    train_full_resnet(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10)\n",
    "else:\n",
    "    print(f\"Found checkpoint '{checkpoint_path}', loading weights...\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# =============================================\n",
    "# 10. Real-Time Inference \n",
    "# =============================================\n",
    "mp_face        = mp.solutions.face_detection\n",
    "face_detection = mp_face.FaceDetection(\n",
    "    model_selection=1,\n",
    "    min_detection_confidence=0.6\n",
    ")\n",
    "emotion_map = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "infer_tfm = val_tfm  # deterministic resize + normalize\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- OPTIONAL STATIC SANITY CHECK THAT I WANTED TO TRY ON THE FER-2013 DATASET ---\n",
    "\n",
    "print(\"\\n=== Running static FER-2013 test batch check ===\")\n",
    "model.eval()\n",
    "imgs, labels = next(iter(test_loader))  \n",
    "imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs)\n",
    "    probs  = F.softmax(logits, dim=1)\n",
    "    preds  = probs.argmax(dim=1)\n",
    "\n",
    "# Print out first 5 examples\n",
    "for i in range(5):\n",
    "    gt_idx   = labels[i].item()\n",
    "    pred_idx = preds[i].item()\n",
    "    prob_vec = probs[i].cpu().numpy().round(2)\n",
    "    print(\n",
    "        f\"Sample {i}: GT={emotion_map[gt_idx]:7s}  \"\n",
    "        f\"Pred={emotion_map[pred_idx]:7s}  \"\n",
    "        f\"Probs={prob_vec}\"\n",
    "    )\n",
    "\n",
    "acc = (preds == labels).sum().item() / labels.size(0)\n",
    "print(f\"Static‐batch accuracy on FER-2013 test batch: {acc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6edb577d-3a70-4780-b9c6-1d5142919d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Cannot open webcam (cv2.VideoCapture failed).\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    ih, iw, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results   = face_detection.process(rgb_frame)\n",
    "\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bbox = det.location_data.relative_bounding_box\n",
    "            x = int(bbox.xmin * iw)\n",
    "            y = int(bbox.ymin * ih)\n",
    "            w = int(bbox.width * iw)\n",
    "            h = int(bbox.height * ih)\n",
    "\n",
    "            # Expanding bbox by 20% for context\n",
    "            m  = 0.2\n",
    "            dx = int(w * m)\n",
    "            dy = int(h * m)\n",
    "            x0 = max(0, x - dx)\n",
    "            y0 = max(0, y - dy)\n",
    "            x1 = min(iw, x + w + dx)\n",
    "            y1 = min(ih, y + h + dy)\n",
    "\n",
    "            face = frame[y0:y1, x0:x1]\n",
    "\n",
    "            # Applying OpenCV preprocessing then normalizing\n",
    "            proc   = preprocess_image(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "            tensor = infer_tfm(proc).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_idx = model(tensor).argmax(1).item()\n",
    "                emo      = emotion_map[pred_idx]\n",
    "\n",
    "            cv2.rectangle(frame, (x0, y0), (x1, y1), (255, 0, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame, emo, (x0, y0 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2\n",
    "            )\n",
    "\n",
    "    cv2.imshow('Real-Time Emotion Detection (only-fc)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Press ESC to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd01f44-4761-4426-b3fd-e7d5bfa2c77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
