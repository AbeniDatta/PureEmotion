{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef9e24c-1aa9-4723-85a1-7fc931d35af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters (should list layer1–4 and fc):\n",
      "  layer1.0.conv1.weight\n",
      "  layer1.0.bn1.weight\n",
      "  layer1.0.bn1.bias\n",
      "  layer1.0.conv2.weight\n",
      "  layer1.0.bn2.weight\n",
      "  layer1.0.bn2.bias\n",
      "  layer1.1.conv1.weight\n",
      "  layer1.1.bn1.weight\n",
      "  layer1.1.bn1.bias\n",
      "  layer1.1.conv2.weight\n",
      "  layer1.1.bn2.weight\n",
      "  layer1.1.bn2.bias\n",
      "  layer2.0.conv1.weight\n",
      "  layer2.0.bn1.weight\n",
      "  layer2.0.bn1.bias\n",
      "  layer2.0.conv2.weight\n",
      "  layer2.0.bn2.weight\n",
      "  layer2.0.bn2.bias\n",
      "  layer2.0.downsample.0.weight\n",
      "  layer2.0.downsample.1.weight\n",
      "  layer2.0.downsample.1.bias\n",
      "  layer2.1.conv1.weight\n",
      "  layer2.1.bn1.weight\n",
      "  layer2.1.bn1.bias\n",
      "  layer2.1.conv2.weight\n",
      "  layer2.1.bn2.weight\n",
      "  layer2.1.bn2.bias\n",
      "  layer3.0.conv1.weight\n",
      "  layer3.0.bn1.weight\n",
      "  layer3.0.bn1.bias\n",
      "  layer3.0.conv2.weight\n",
      "  layer3.0.bn2.weight\n",
      "  layer3.0.bn2.bias\n",
      "  layer3.0.downsample.0.weight\n",
      "  layer3.0.downsample.1.weight\n",
      "  layer3.0.downsample.1.bias\n",
      "  layer3.1.conv1.weight\n",
      "  layer3.1.bn1.weight\n",
      "  layer3.1.bn1.bias\n",
      "  layer3.1.conv2.weight\n",
      "  layer3.1.bn2.weight\n",
      "  layer3.1.bn2.bias\n",
      "  layer4.0.conv1.weight\n",
      "  layer4.0.bn1.weight\n",
      "  layer4.0.bn1.bias\n",
      "  layer4.0.conv2.weight\n",
      "  layer4.0.bn2.weight\n",
      "  layer4.0.bn2.bias\n",
      "  layer4.0.downsample.0.weight\n",
      "  layer4.0.downsample.1.weight\n",
      "  layer4.0.downsample.1.bias\n",
      "  layer4.1.conv1.weight\n",
      "  layer4.1.bn1.weight\n",
      "  layer4.1.bn1.bias\n",
      "  layer4.1.conv2.weight\n",
      "  layer4.1.bn2.weight\n",
      "  layer4.1.bn2.bias\n",
      "  fc.weight\n",
      "  fc.bias\n",
      "Epoch 1/2  Train L: 1.4663, Acc: 0.4356  |  Val L: 1.2347, Acc: 0.5344\n",
      "Epoch 2/2  Train L: 1.1997, Acc: 0.5474  |  Val L: 1.1124, Acc: 0.5715\n",
      "Best Val Acc: 0.5715 (weights saved to full_resnet_emotion.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748891904.338700 19963585 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748891904.377072 20092428 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running static FER-2013 test batch check ===\n",
      "Sample 0: GT=Angry    Pred=Fear     Probs=[0.22 0.02 0.34 0.01 0.26 0.03 0.13]\n",
      "Sample 1: GT=Surprise  Pred=Sad      Probs=[0.06 0.   0.27 0.03 0.5  0.04 0.1 ]\n",
      "Sample 2: GT=Neutral  Pred=Sad      Probs=[0.08 0.   0.29 0.02 0.43 0.01 0.16]\n",
      "Sample 3: GT=Sad      Pred=Fear     Probs=[0.21 0.   0.34 0.   0.34 0.05 0.06]\n",
      "Sample 4: GT=Fear     Pred=Angry    Probs=[0.37 0.02 0.27 0.01 0.27 0.05 0.02]\n",
      "Static‐batch accuracy on FER-2013 test batch: 46.88%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Abeni's Real-Time Emotion Detection Model made with PyTorch, Mediapipe, ResNet-18 and OpenCV\n",
    "# (Without CLAHE and Adaptive Thresholding)\n",
    "#---------------------------\n",
    "\n",
    "# =============================================\n",
    "# 1. All Imports\n",
    "# =============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# =============================================\n",
    "# 2. Dataset Definition\n",
    "# =============================================\n",
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, csv_file, usage=\"Training\", transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data[self.data[\"Usage\"] == usage].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row    = self.data.iloc[idx]\n",
    "        pixels = np.fromstring(row[\"pixels\"], dtype=int, sep=' ')\n",
    "        img    = pixels.reshape(48,48).astype(np.uint8)\n",
    "        img    = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = int(row[\"emotion\"])\n",
    "        return img, label\n",
    "\n",
    "# =============================================\n",
    "# 3. Data Transforms\n",
    "# =============================================\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "val_tfm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# 4. Data Loaders\n",
    "# =============================================\n",
    "train_ds  = FER2013Dataset('fer2013.csv', usage='Training',   transform=train_tfm)\n",
    "val_ds    = FER2013Dataset('fer2013.csv', usage='PublicTest', transform=val_tfm)\n",
    "test_ds   = FER2013Dataset('fer2013.csv', usage='PrivateTest',transform=val_tfm)\n",
    "\n",
    "train_loader = DataLoader(train_ds,  batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,    batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# =============================================\n",
    "# 5. Model Definition (unfreezing layers 1 to 4 + fc)\n",
    "# =============================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Unfreeze layer1, layer2, layer3, layer4, and fc\n",
    "for name, param in model.named_parameters():\n",
    "    if not (\n",
    "        name.startswith('layer1') or\n",
    "        name.startswith('layer2') or\n",
    "        name.startswith('layer3') or\n",
    "        name.startswith('layer4') or\n",
    "        name.startswith('fc')\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Replace final fc (1000 -> 7)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 7)\n",
    "model    = model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable parameters (should list layer1–4 and fc):\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name}\")\n",
    "\n",
    "# =============================================\n",
    "# 6. Training Setup (separate LRs + scheduler)\n",
    "# =============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params_to_update = [\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.fc.parameters(),     'lr': 1e-4}\n",
    "]\n",
    "optimizer = optim.Adam(params_to_update)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# =============================================\n",
    "# 7. Training Loop (fine‐tuning all of layers 1 to 4 + fc)\n",
    "# =============================================\n",
    "def train_full_resnet(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=2):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss, running_corr = 0.0, 0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss    += loss.item() * inputs.size(0)\n",
    "            running_corr    += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc  = running_corr / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_corr = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss    = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corr += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc  = val_corr / len(val_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs}  \"\n",
    "            f\"Train L: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}  |  \"\n",
    "            f\"Val L: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'full_resnet_emotion.pth')\n",
    "\n",
    "    print(f\"Best Val Acc: {best_acc:.4f} (weights saved to full_resnet_emotion.pth)\")\n",
    "\n",
    "# =============================================\n",
    "# 8. Run Training & Save Weights\n",
    "# =============================================\n",
    "# Remove any outdated checkpoints so we retrain from scratch\n",
    "for old_fn in ['only_fc_emotion_resnet.pth', 'layer2_to_fc_emotion_resnet.pth', 'full_resnet_emotion.pth']:\n",
    "    if os.path.exists(old_fn):\n",
    "        os.remove(old_fn)\n",
    "\n",
    "checkpoint_path = 'full_resnet_emotion.pth'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    train_full_resnet(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=2)\n",
    "else:\n",
    "    print(f\"Found checkpoint '{checkpoint_path}', loading weights...\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# =============================================\n",
    "# 9. Real-Time Inference \n",
    "# =============================================\n",
    "mp_face        = mp.solutions.face_detection\n",
    "face_detection = mp_face.FaceDetection(\n",
    "    model_selection=1,\n",
    "    min_detection_confidence=0.6\n",
    ")\n",
    "emotion_map = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "infer_tfm = val_tfm  # deterministic resize + normalize\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- OPTIONAL STATIC SANITY CHECK THAT I WANTED TO TRY ON THE FER-2013 DATASET ---\n",
    "\n",
    "print(\"\\n=== Running static FER-2013 test batch check ===\")\n",
    "model.eval()\n",
    "imgs, labels = next(iter(test_loader))  # grab one batch from PrivateTest or PublicTest\n",
    "imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs)\n",
    "    probs  = F.softmax(logits, dim=1)\n",
    "    preds  = probs.argmax(dim=1)\n",
    "\n",
    "# Print out first 5 examples\n",
    "for i in range(5):\n",
    "    gt_idx   = labels[i].item()\n",
    "    pred_idx = preds[i].item()\n",
    "    prob_vec = probs[i].cpu().numpy().round(2)\n",
    "    print(\n",
    "        f\"Sample {i}: GT={emotion_map[gt_idx]:7s}  \"\n",
    "        f\"Pred={emotion_map[pred_idx]:7s}  \"\n",
    "        f\"Probs={prob_vec}\"\n",
    "    )\n",
    "\n",
    "acc = (preds == labels).sum().item() / labels.size(0)\n",
    "print(f\"Static‐batch accuracy on FER-2013 test batch: {acc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f0007-118e-426d-bef1-0d61d5fe0ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
